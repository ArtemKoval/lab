{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749,
          "referenced_widgets": [
            "2f0fab619f75422594faf90a3395ccdd",
            "d1c50c56e031476295bbb36af3a57d23",
            "4101c895debe423d945d1e0448e1f1dc",
            "3edf7440f4ab47bf804fc26fdbcaf303",
            "0d05b5fe622a4ff98fd7d6985fb6a356",
            "82a00217f350471ba9319b9138bbe3e0",
            "85f92b0fdcc14f8289ed77466f9e97a2",
            "1024ea6f5e5445bf8c29cf6a0b802e54",
            "96b9e1686f244ffc931a7c705064a62a",
            "e29a6ea3a00545058e84d70a801293c7",
            "50314b401d2e401cada5d7b842d147f3"
          ]
        },
        "id": "K6RVWhy-Iy36",
        "outputId": "50c2c7be-e189-4e9f-93ef-6f94ed7580e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Python dependencies...\n",
            "Python dependencies installed.\n",
            "Installing Ollama...\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Ollama installation completed.\n",
            "Spawning background thread to start Ollama server...\n",
            "Starting Ollama server...\n",
            "Ollama is running in the background.\n",
            "Pulling gemma3:1b model...\n",
            "Successfully pulled gemma3:1b\n",
            "Pulling bge-m3 embedding model...\n",
            "Successfully pulled bge-m3\n",
            "Setting up LangChain and Ragas components...\n",
            "Initializing OllamaEmbeddings with bge-m3...\n",
            "Embedding wrapper initialized.\n",
            "Initializing ChatOllama with gemma3:1b (debug mode)...\n",
            "LLM wrapper initialized.\n",
            "Assigning LLM and embeddings to evaluation metrics...\n",
            "Setting LLM for metric: AnswerCorrectness(_required_columns={<MetricType.SINGLE_TURN: 'single_turn'>: {'response', 'reference', 'user_input'}}, name='answer_correctness', embeddings=LangchainEmbeddingsWrapper(embeddings=OllamaEmbeddings(...)), llm=LangchainLLMWrapper(langchain_llm=ChatOllama(...)), output_type=None, correctness_prompt=CorrectnessClassifier(instruction=Given a ground truth and an answer statements, analyze each statement and classify them in one of the following categories: TP (true positive): statements that are present in answer that are also directly supported by the one or more statements in ground truth, FP (false positive): statements present in the answer but not directly supported by any statement in ground truth, FN (false negative): statements found in the ground truth but not present in answer. Each statement can only belong to one of the categories. Provide a reason for each classification., examples=[(QuestionAnswerGroundTruth(question='What powers the sun and what is its primary function?', answer=['The sun is powered by nuclear fission, similar to nuclear reactors on Earth.', 'The primary function of the sun is to provide light to the solar system.'], ground_truth=['The sun is powered by nuclear fusion, where hydrogen atoms fuse to form helium.', \"This fusion process in the sun's core releases a tremendous amount of energy.\", 'The energy from the sun provides heat and light, which are essential for life on Earth.', \"The sun's light plays a critical role in Earth's climate system.\", 'Sunlight helps to drive the weather and ocean currents.']), ClassificationWithReason(TP=[StatementsWithReason(statement='The primary function of the sun is to provide light to the solar system.', reason=\"This statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy.\")], FP=[StatementsWithReason(statement='The sun is powered by nuclear fission, similar to nuclear reactors on Earth.', reason='This statement is incorrect and contradicts the ground truth which states that the sun is powered by nuclear fusion.')], FN=[StatementsWithReason(statement='The sun is powered by nuclear fusion, where hydrogen atoms fuse to form helium.', reason='This accurate description of the sun’s power source is not included in the answer.'), StatementsWithReason(statement=\"This fusion process in the sun's core releases a tremendous amount of energy.\", reason='This process and its significance are not mentioned in the answer.'), StatementsWithReason(statement='The energy from the sun provides heat and light, which are essential for life on Earth.', reason='The answer only mentions light, omitting the essential aspects of heat and its necessity for life, which the ground truth covers.'), StatementsWithReason(statement=\"The sun's light plays a critical role in Earth's climate system.\", reason=\"This broader impact of the sun’s light on Earth's climate system is not addressed in the answer.\"), StatementsWithReason(statement='Sunlight helps to drive the weather and ocean currents.', reason='The effect of sunlight on weather patterns and ocean currents is omitted in the answer.')])), (QuestionAnswerGroundTruth(question='What is the boiling point of water?', answer=['The boiling point of water is 100 degrees Celsius at sea level'], ground_truth=['The boiling point of water is 100 degrees Celsius (212 degrees Fahrenheit) at sea level.', 'The boiling point of water can change with altitude.']), ClassificationWithReason(TP=[StatementsWithReason(statement='The boiling point of water is 100 degrees Celsius at sea level', reason='This statement is directly supported by the ground truth which specifies the boiling point of water as 100 degrees Celsius at sea level.')], FP=[], FN=[StatementsWithReason(statement='The boiling point of water can change with altitude.', reason='This additional information about how the boiling point of water can vary with altitude is not mentioned in the answer.')]))], language=english), statement_generator_prompt=StatementGeneratorPrompt(instruction=Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON., examples=[(StatementGeneratorInput(question='Who was Albert Einstein and what is he best known for?', answer='He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.'), StatementGeneratorOutput(statements=['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.', 'Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']))], language=english), weights=[0.75, 0.25], beta=1.0, answer_similarity=None, max_retries=1)\n",
            "Setting embeddings for metric: AnswerCorrectness(_required_columns={<MetricType.SINGLE_TURN: 'single_turn'>: {'response', 'reference', 'user_input'}}, name='answer_correctness', embeddings=LangchainEmbeddingsWrapper(embeddings=OllamaEmbeddings(...)), llm=LangchainLLMWrapper(langchain_llm=DebuggingLLM(...)), output_type=None, correctness_prompt=CorrectnessClassifier(instruction=Given a ground truth and an answer statements, analyze each statement and classify them in one of the following categories: TP (true positive): statements that are present in answer that are also directly supported by the one or more statements in ground truth, FP (false positive): statements present in the answer but not directly supported by any statement in ground truth, FN (false negative): statements found in the ground truth but not present in answer. Each statement can only belong to one of the categories. Provide a reason for each classification., examples=[(QuestionAnswerGroundTruth(question='What powers the sun and what is its primary function?', answer=['The sun is powered by nuclear fission, similar to nuclear reactors on Earth.', 'The primary function of the sun is to provide light to the solar system.'], ground_truth=['The sun is powered by nuclear fusion, where hydrogen atoms fuse to form helium.', \"This fusion process in the sun's core releases a tremendous amount of energy.\", 'The energy from the sun provides heat and light, which are essential for life on Earth.', \"The sun's light plays a critical role in Earth's climate system.\", 'Sunlight helps to drive the weather and ocean currents.']), ClassificationWithReason(TP=[StatementsWithReason(statement='The primary function of the sun is to provide light to the solar system.', reason=\"This statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy.\")], FP=[StatementsWithReason(statement='The sun is powered by nuclear fission, similar to nuclear reactors on Earth.', reason='This statement is incorrect and contradicts the ground truth which states that the sun is powered by nuclear fusion.')], FN=[StatementsWithReason(statement='The sun is powered by nuclear fusion, where hydrogen atoms fuse to form helium.', reason='This accurate description of the sun’s power source is not included in the answer.'), StatementsWithReason(statement=\"This fusion process in the sun's core releases a tremendous amount of energy.\", reason='This process and its significance are not mentioned in the answer.'), StatementsWithReason(statement='The energy from the sun provides heat and light, which are essential for life on Earth.', reason='The answer only mentions light, omitting the essential aspects of heat and its necessity for life, which the ground truth covers.'), StatementsWithReason(statement=\"The sun's light plays a critical role in Earth's climate system.\", reason=\"This broader impact of the sun’s light on Earth's climate system is not addressed in the answer.\"), StatementsWithReason(statement='Sunlight helps to drive the weather and ocean currents.', reason='The effect of sunlight on weather patterns and ocean currents is omitted in the answer.')])), (QuestionAnswerGroundTruth(question='What is the boiling point of water?', answer=['The boiling point of water is 100 degrees Celsius at sea level'], ground_truth=['The boiling point of water is 100 degrees Celsius (212 degrees Fahrenheit) at sea level.', 'The boiling point of water can change with altitude.']), ClassificationWithReason(TP=[StatementsWithReason(statement='The boiling point of water is 100 degrees Celsius at sea level', reason='This statement is directly supported by the ground truth which specifies the boiling point of water as 100 degrees Celsius at sea level.')], FP=[], FN=[StatementsWithReason(statement='The boiling point of water can change with altitude.', reason='This additional information about how the boiling point of water can vary with altitude is not mentioned in the answer.')]))], language=english), statement_generator_prompt=StatementGeneratorPrompt(instruction=Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON., examples=[(StatementGeneratorInput(question='Who was Albert Einstein and what is he best known for?', answer='He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.'), StatementGeneratorOutput(statements=['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.', 'Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']))], language=english), weights=[0.75, 0.25], beta=1.0, answer_similarity=None, max_retries=1)\n",
            "Setting LLM for metric: AnswerSimilarity(_required_columns={<MetricType.SINGLE_TURN: 'single_turn'>: {'response', 'reference'}}, name='semantic_similarity', embeddings=LangchainEmbeddingsWrapper(embeddings=OllamaEmbeddings(...)), is_cross_encoder=False, threshold=None)\n",
            "Setting embeddings for metric: AnswerSimilarity(_required_columns={<MetricType.SINGLE_TURN: 'single_turn'>: {'response', 'reference'}}, name='semantic_similarity', embeddings=LangchainEmbeddingsWrapper(embeddings=OllamaEmbeddings(...)), is_cross_encoder=False, threshold=None)\n",
            "Metrics setup complete.\n",
            "Creating evaluation dataset...\n",
            "Dataset created.\n",
            "Starting evaluation process...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f0fab619f75422594faf90a3395ccdd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation completed.\n",
            "\n",
            "=== Evaluation Results ===\n",
            "{'answer_correctness': 0.5184, 'semantic_similarity': 0.7235}\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install required packages\n",
        "print(\"Installing Python dependencies...\")\n",
        "!pip install -q ollama ragas langchain langchain-community langchain-ollama\n",
        "print(\"Python dependencies installed.\")\n",
        "\n",
        "# Step 2: Install Ollama\n",
        "print(\"Installing Ollama...\")\n",
        "!curl -fsSL https://ollama.com/install.sh  | sh\n",
        "print(\"Ollama installation completed.\")\n",
        "\n",
        "# Step 3: Start Ollama server in the background\n",
        "import threading\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "def run_ollama_serve():\n",
        "    print(\"Starting Ollama server...\")\n",
        "    subprocess.run([\"ollama\", \"serve\"])\n",
        "\n",
        "print(\"Spawning background thread to start Ollama server...\")\n",
        "threading.Thread(target=run_ollama_serve).start()\n",
        "time.sleep(5)  # Wait for Ollama to initialize\n",
        "print(\"Ollama is running in the background.\")\n",
        "\n",
        "# Step 4: Pull gemma3:1b and bge-m3 models\n",
        "import ollama\n",
        "\n",
        "print(\"Pulling gemma3:1b model...\")\n",
        "ollama.pull(\"gemma3:1b\")\n",
        "print(\"Successfully pulled gemma3:1b\")\n",
        "\n",
        "print(\"Pulling bge-m3 embedding model...\")\n",
        "ollama.pull(\"bge-m3\")\n",
        "print(\"Successfully pulled bge-m3\")\n",
        "\n",
        "# Step 5: Set up LangChain + Ragas integration\n",
        "print(\"Setting up LangChain and Ragas components...\")\n",
        "\n",
        "import os\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
        "from langchain_core.messages import HumanMessage\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas import evaluate\n",
        "from datasets import Dataset\n",
        "from ragas.metrics import answer_correctness, answer_similarity\n",
        "\n",
        "# Dummy OpenAI key (to avoid errors)\n",
        "os.environ['OPENAI_API_KEY'] = 'no-key'\n",
        "\n",
        "# Custom wrapper to log input prompts and model outputs\n",
        "class DebuggingLLM(ChatOllama):\n",
        "    def invoke(self, input, *args, **kwargs):\n",
        "        # Log prompt\n",
        "        if isinstance(input, HumanMessage):\n",
        "            prompt = input.content\n",
        "        else:\n",
        "            prompt = str(input)\n",
        "\n",
        "        print(\"\\n[DEBUG] Prompt sent to LLM:\")\n",
        "        print(prompt[:500] + \"...\" if len(prompt) > 500 else prompt)\n",
        "\n",
        "        # Call original LLM\n",
        "        response = super().invoke(input, *args, **kwargs)\n",
        "\n",
        "        # Log response\n",
        "        output_text = response.content\n",
        "        print(\"\\n[DEBUG] LLM Response:\")\n",
        "        print(output_text[:500] + \"...\" if len(output_text) > 500 else output_text)\n",
        "\n",
        "        return response\n",
        "\n",
        "\n",
        "# Initialize Embeddings\n",
        "print(\"Initializing OllamaEmbeddings with bge-m3...\")\n",
        "ollama_e = OllamaEmbeddings(model=\"bge-m3\", base_url=\"http://localhost:11434\")\n",
        "ollama_embed_self = LangchainEmbeddingsWrapper(ollama_e)\n",
        "print(\"Embedding wrapper initialized.\")\n",
        "\n",
        "# Initialize LLM (gemma3:1b) using our debugging wrapper\n",
        "print(\"Initializing ChatOllama with gemma3:1b (debug mode)...\")\n",
        "ollama_llm = DebuggingLLM(model=\"gemma3:1b\", base_url=\"http://localhost:11434\")\n",
        "wrapper = LangchainLLMWrapper(ollama_llm)\n",
        "print(\"LLM wrapper initialized.\")\n",
        "\n",
        "# Assign LLM and embeddings to metrics\n",
        "print(\"Assigning LLM and embeddings to evaluation metrics...\")\n",
        "metrics = [answer_correctness, answer_similarity]\n",
        "for m in metrics:\n",
        "    print(f\"Setting LLM for metric: {m}\")\n",
        "    m.llm = wrapper\n",
        "    if hasattr(m, \"embeddings\"):\n",
        "        print(f\"Setting embeddings for metric: {m}\")\n",
        "        m.embeddings = ollama_embed_self\n",
        "print(\"Metrics setup complete.\")\n",
        "\n",
        "# Example data (replace these with your real data)\n",
        "questions = [\"What is the capital of France?\", \"Explain quantum computing.\"]\n",
        "labels = [\"Paris is the capital of France.\", \"Quantum computing uses qubits.\"]\n",
        "naive_rag_answers = [\"Paris\", \"It uses qubits.\"]\n",
        "\n",
        "# Create dataset\n",
        "print(\"Creating evaluation dataset...\")\n",
        "dataset = Dataset.from_dict({\n",
        "    \"question\": questions,\n",
        "    \"ground_truth\": labels,\n",
        "    \"answer\": naive_rag_answers,\n",
        "})\n",
        "print(\"Dataset created.\")\n",
        "\n",
        "# Evaluate\n",
        "print(\"Starting evaluation process...\")\n",
        "results = evaluate(\n",
        "    dataset,\n",
        "    metrics=metrics,\n",
        "    embeddings=ollama_embed_self,\n",
        "    llm=wrapper\n",
        ")\n",
        "print(\"Evaluation completed.\")\n",
        "\n",
        "# Print results\n",
        "print(\"\\n=== Evaluation Results ===\")\n",
        "print(results)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f0fab619f75422594faf90a3395ccdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1c50c56e031476295bbb36af3a57d23",
              "IPY_MODEL_4101c895debe423d945d1e0448e1f1dc",
              "IPY_MODEL_3edf7440f4ab47bf804fc26fdbcaf303"
            ],
            "layout": "IPY_MODEL_0d05b5fe622a4ff98fd7d6985fb6a356"
          }
        },
        "d1c50c56e031476295bbb36af3a57d23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82a00217f350471ba9319b9138bbe3e0",
            "placeholder": "​",
            "style": "IPY_MODEL_85f92b0fdcc14f8289ed77466f9e97a2",
            "value": "Evaluating: 100%"
          }
        },
        "4101c895debe423d945d1e0448e1f1dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1024ea6f5e5445bf8c29cf6a0b802e54",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96b9e1686f244ffc931a7c705064a62a",
            "value": 4
          }
        },
        "3edf7440f4ab47bf804fc26fdbcaf303": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e29a6ea3a00545058e84d70a801293c7",
            "placeholder": "​",
            "style": "IPY_MODEL_50314b401d2e401cada5d7b842d147f3",
            "value": " 4/4 [00:06&lt;00:00,  1.70s/it]"
          }
        },
        "0d05b5fe622a4ff98fd7d6985fb6a356": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82a00217f350471ba9319b9138bbe3e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85f92b0fdcc14f8289ed77466f9e97a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1024ea6f5e5445bf8c29cf6a0b802e54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96b9e1686f244ffc931a7c705064a62a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e29a6ea3a00545058e84d70a801293c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50314b401d2e401cada5d7b842d147f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}