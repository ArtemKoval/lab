{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJnDMtYtATrA",
        "outputId": "6fd04c6b-5ffa-4cd0-f507-99b9c57209b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT Classification Result:\n",
            "[{'label': 'LABEL_1', 'score': 0.9978476762771606}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 Generation Result:\n",
            "Artificial intelligence is changing the way we think about a very large number of things. It's making it a lot more difficult to make good decisions.\n",
            "\n",
            "As technology goes along that isn't going to be a new phenomenon. It will continue to\n",
            "\n",
            "\n",
            "\n",
            "=== Conceptual Overview: Reinforcement Learning from Human Feedback (RLHF) ===\n",
            "\n",
            "While not demonstrated here due to computational complexity, RLHF involves:\n",
            "\n",
            "1. Supervised Fine-Tuning (SFT): Training a base model on human-written responses.\n",
            "2. Reward Model Training: Building a model that scores outputs based on human preferences.\n",
            "3. Policy Optimization: Updating the language model using reinforcement learning to maximize reward.\n",
            "\n",
            "This technique has been used by OpenAI to align models like ChatGPT with user expectations in conversational settings.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# INSTALL REQUIRED PACKAGES\n",
        "!pip -q install transformers torch\n",
        "\n",
        "# IMPORT LIBRARIES\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForCausalLM,\n",
        "    pipeline\n",
        ")\n",
        "import torch\n",
        "\n",
        "# -----------------------------\n",
        "# BERT FOR TEXT CLASSIFICATION\n",
        "# -----------------------------\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer (fine-tuned on SST-2 for sentiment analysis)\n",
        "model_name_bert = \"bert-base-uncased\"\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(model_name_bert)\n",
        "model_bert = AutoModelForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
        "\n",
        "# Create a classification pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model_bert, tokenizer=tokenizer_bert)\n",
        "\n",
        "# Run inference\n",
        "result_bert = classifier(\"I love using Google Colab for deep learning projects.\")\n",
        "print(\"BERT Classification Result:\")\n",
        "print(result_bert)\n",
        "\n",
        "# -----------------------------\n",
        "# GPT-2 FOR TEXT GENERATION\n",
        "# -----------------------------\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "model_name_gpt = \"gpt2\"\n",
        "tokenizer_gpt = AutoTokenizer.from_pretrained(model_name_gpt)\n",
        "model_gpt = AutoModelForCausalLM.from_pretrained(model_name_gpt)\n",
        "\n",
        "# Create a text generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=model_gpt, tokenizer=tokenizer_gpt)\n",
        "\n",
        "# Generate text\n",
        "prompt_gpt = \"Artificial intelligence is changing the way we\"\n",
        "result_gpt = generator(prompt_gpt, max_length=50, num_return_sequences=1)\n",
        "\n",
        "# Access the generated text safely\n",
        "print(\"\\nGPT-2 Generation Result:\")\n",
        "if isinstance(result_gpt, list) and len(result_gpt) > 0:\n",
        "    print(result_gpt[0].get('generated_text', 'No generated text found'))\n",
        "\n",
        "# -----------------------------\n",
        "# RLHF DISCUSSION (Conceptual)\n",
        "# -----------------------------\n",
        "\n",
        "print(\"\"\"\n",
        "\\n\n",
        "=== Conceptual Overview: Reinforcement Learning from Human Feedback (RLHF) ===\n",
        "\n",
        "While not demonstrated here due to computational complexity, RLHF involves:\n",
        "\n",
        "1. Supervised Fine-Tuning (SFT): Training a base model on human-written responses.\n",
        "2. Reward Model Training: Building a model that scores outputs based on human preferences.\n",
        "3. Policy Optimization: Updating the language model using reinforcement learning to maximize reward.\n",
        "\n",
        "This technique has been used by OpenAI to align models like ChatGPT with user expectations in conversational settings.\n",
        "\"\"\")"
      ]
    }
  ]
}