{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbQhxjg5C5ne"
      },
      "outputs": [],
      "source": [
        "Here is the full **Google Colab notebook code** combined into a single, executable script. You can copy and paste this directly into a Colab notebook cell to run the entire **Retrieval-Augmented Generation (RAG)** demo.\n",
        "\n",
        "> üîê **Before running:**\n",
        "- Replace `\"your_huggingface_api_token\"` with your actual Hugging Face token from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "- Upload a PDF file when prompted after running the notebook\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "# Step 1: Install required libraries\n",
        "!pip install -q langchain faiss-cpu transformers sentence-transformers pypdf openai\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import os\n",
        "from google.colab import files\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "# Step 3: Upload and load PDF document\n",
        "print(\"Please upload a PDF file.\")\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "loader = PyPDFLoader(file_name)\n",
        "documents = loader.load()\n",
        "\n",
        "# Split document into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Step 4: Create embeddings and build vector store\n",
        "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "db = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "# Step 5: Load a pre-trained language model (Hugging Face Hub)\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"your_huggingface_api_token\"\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    model_kwargs={\"temperature\": 0.5, \"max_length\": 512}\n",
        ")\n",
        "\n",
        "# Step 6: Build the RAG chain\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=db.as_retriever())\n",
        "\n",
        "# Step 7: Define question function and ask sample question\n",
        "def ask_question(question):\n",
        "    response = qa_chain.run(question)\n",
        "    print(\"Answer:\", response)\n",
        "\n",
        "ask_question(\"What are the key findings in this document?\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "This complete notebook performs the following:\n",
        "- Installs necessary dependencies\n",
        "- Loads and processes a user-uploaded PDF\n",
        "- Splits content into chunks\n",
        "- Embeds and stores them using FAISS\n",
        "- Uses a hosted LLM via Hugging Face\n",
        "- Sets up and runs a RAG-based QA chain\n",
        "\n",
        "You can modify the `ask_question()` call to test different queries based on your uploaded document."
      ]
    }
  ]
}