{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "all25BYCgwia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "399fbbd7-3e91-4ba1-b137-2e3588838f62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running AI PM simulation with user feedback...\n",
            "\n",
            "\n",
            "Input: I absolutely love this product!\n",
            "Model Response:\n",
            "Positive, 1. The user input clearly expresses a strong positive sentiment towards the product, using words like \"absolutely love\". There is no indication of any negative feelings or criticisms, leading to a high confidence in labeling this sentiment as positive.\n",
            "Is this accurate? (yes/no): yes\n",
            "\n",
            "Input: This service is terrible and slow.\n",
            "Model Response:\n",
            "Negative, 1.0\n",
            "\n",
            "The user input clearly expresses dissatisfaction with the service, using strong negative language such as \"terrible\" and \"slow.\" There is no indication of any positive aspects mentioned, leading to a confident classification of negative sentiment.\n",
            "Is this accurate? (yes/no): yes\n",
            "\n",
            "Input: It's fine, but I’ve seen better.\n",
            "Model Response:\n",
            "Negative, 0.8. The user is expressing disappointment by saying they have seen better, indicating that they were not completely satisfied with whatever they are referring to.\n",
            "Is this accurate? (yes/no): yes\n",
            "\n",
            "Input: I can’t decide how I feel about this.\n",
            "Model Response:\n",
            "Neutral, 0.8. The user expresses uncertainty and indecision about their feelings, which suggests a neutral sentiment. The use of the word \"can't\" also indicates a lack of strong positive or negative emotions.\n",
            "Is this accurate? (yes/no): yes\n",
            "\n",
            "Input: Worst purchase I’ve made.\n",
            "Model Response:\n",
            "Negative, 1.0. The user explicitly states that their purchase was the worst, indicating a strong negative sentiment.\n",
            "Is this accurate? (yes/no): yes\n",
            "\n",
            "--- Feedback Summary ---\n",
            "Total inputs: 5\n",
            "User-flagged misclassifications: 0\n",
            "\n",
            "--- Product Manager Notes ---\n",
            "- Review edge cases where confidence is low or sentiment is mixed\n",
            "- Adjust prompts or explore prompt chaining for nuanced input\n",
            "- Provide explanations and fallback UX for uncertain outputs\n",
            "- Iterate based on flagged examples or expand training data\n"
          ]
        }
      ],
      "source": [
        "# Combined AI PM Demo with updated OpenAI API for version 1.0+\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Initialize client with key from Colab secrets\n",
        "api_key = userdata.get('OPEN_API_KEY')\n",
        "if not api_key:\n",
        "    raise ValueError(\"Missing OPENAI_API_KEY. Set it in Colab secrets (left sidebar).\")\n",
        "\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "def get_sentiment_with_confidence(text):\n",
        "    prompt = (\n",
        "        f\"Analyze the sentiment of this user input: '{text}'\\n\"\n",
        "        \"Respond with one of [Positive, Negative, Neutral] and explain your confidence (0 to 1).\"\n",
        "    )\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.2\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def simulate_feedback(text):\n",
        "    print(f\"\\nInput: {text}\")\n",
        "    model_response = get_sentiment_with_confidence(text)\n",
        "    print(f\"Model Response:\\n{model_response}\")\n",
        "    feedback = input(\"Is this accurate? (yes/no): \").strip().lower()\n",
        "    return {\"text\": text, \"response\": model_response, \"feedback\": feedback}\n",
        "\n",
        "examples = [\n",
        "    \"I absolutely love this product!\",\n",
        "    \"This service is terrible and slow.\",\n",
        "    \"It's fine, but I’ve seen better.\",\n",
        "    \"I can’t decide how I feel about this.\",\n",
        "    \"Worst purchase I’ve made.\"\n",
        "]\n",
        "\n",
        "log = []\n",
        "print(\"Running AI PM simulation with user feedback...\\n\")\n",
        "for example in examples:\n",
        "    log.append(simulate_feedback(example))\n",
        "\n",
        "print(\"\\n--- Feedback Summary ---\")\n",
        "errors = [entry for entry in log if entry['feedback'] == 'no']\n",
        "print(f\"Total inputs: {len(log)}\")\n",
        "print(f\"User-flagged misclassifications: {len(errors)}\")\n",
        "\n",
        "if errors:\n",
        "    print(\"\\nExamples needing improvement:\")\n",
        "    for entry in errors:\n",
        "        print(f\"\\nText: {entry['text']}\")\n",
        "        print(f\"Model Reply: {entry['response']}\")\n",
        "        print(\"Feedback: User marked as inaccurate\")\n",
        "\n",
        "print(\"\\n--- Product Manager Notes ---\")\n",
        "print(\"- Review edge cases where confidence is low or sentiment is mixed\")\n",
        "print(\"- Adjust prompts or explore prompt chaining for nuanced input\")\n",
        "print(\"- Provide explanations and fallback UX for uncertain outputs\")\n",
        "print(\"- Iterate based on flagged examples or expand training data\")\n"
      ]
    }
  ]
}