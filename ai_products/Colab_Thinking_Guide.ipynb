{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Rf7uxWGuudj",
        "outputId": "bedd37dc-7834-4989-b9fe-837b5f544511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running AI PM simulation with user feedback...\n",
            "\n",
            "\n",
            "Input: I absolutely love this product!\n",
            "Model Response:\n",
            "Positive, 1. This user input clearly expresses a strong positive sentiment towards the product with the use of the word \"love\" and the intensifier \"absolutely\". The exclamation mark also indicates excitement and satisfaction.\n",
            "Is this accurate? (yes/no): yes\n",
            "\n",
            "Input: This service is terrible and slow.\n",
            "Model Response:\n",
            "Negative, 1.0\n",
            "\n",
            "The user input clearly expresses dissatisfaction with the service, describing it as \"terrible and slow.\" The use of such negative language indicates a strong negative sentiment.\n",
            "Is this accurate? (yes/no): yes\n",
            "\n",
            "Input: It's fine, but I’ve seen better.\n",
            "Model Response:\n",
            "Negative, 0.8. The user is expressing disappointment by saying they have seen better, indicating that they were not fully satisfied with whatever they are referring to.\n",
            "Is this accurate? (yes/no): yes\n",
            "\n",
            "Input: I can’t decide how I feel about this.\n",
            "Model Response:\n",
            "Neutral, 0.8. The user expresses uncertainty and indecision about their feelings, which suggests a neutral sentiment. The use of the word \"can't\" also implies a lack of strong positive or negative emotions.\n"
          ]
        }
      ],
      "source": [
        "# AI PM Demo using OpenAI API and Colab userdata secrets\n",
        "\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import sys\n",
        "\n",
        "# Securely get API key\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    sys.exit(\"Missing OPENAI_API_KEY. Add it via Colab 'Secrets' tab.\")\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# GPT sentiment function with explanation\n",
        "def get_sentiment_with_confidence(text):\n",
        "    prompt = (\n",
        "        f\"Analyze the sentiment of this user input: '{text}'\\n\"\n",
        "        \"Respond with one of [Positive, Negative, Neutral] and explain your confidence (0 to 1).\"\n",
        "    )\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.2\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# Simulated feedback loop with user judgment\n",
        "def simulate_feedback(text):\n",
        "    print(f\"\\nInput: {text}\")\n",
        "    model_response = get_sentiment_with_confidence(text)\n",
        "    print(f\"Model Response:\\n{model_response}\")\n",
        "    feedback = input(\"Is this accurate? (yes/no): \").strip().lower()\n",
        "    return {\"text\": text, \"response\": model_response, \"feedback\": feedback}\n",
        "\n",
        "# Example product feedback texts\n",
        "examples = [\n",
        "    \"I absolutely love this product!\",\n",
        "    \"This service is terrible and slow.\",\n",
        "    \"It's fine, but I’ve seen better.\",\n",
        "    \"I can’t decide how I feel about this.\",\n",
        "    \"Worst purchase I’ve made.\"\n",
        "]\n",
        "\n",
        "log = []\n",
        "print(\"Running AI PM simulation with user feedback...\\n\")\n",
        "for example in examples:\n",
        "    log.append(simulate_feedback(example))\n",
        "\n",
        "# Review flagged model behavior\n",
        "print(\"\\n--- Feedback Summary ---\")\n",
        "errors = [entry for entry in log if entry['feedback'] == 'no']\n",
        "print(f\"Total inputs: {len(log)}\")\n",
        "print(f\"User-flagged misclassifications: {len(errors)}\")\n",
        "\n",
        "if errors:\n",
        "    print(\"\\nExamples needing review:\")\n",
        "    for entry in errors:\n",
        "        print(f\"\\nText: {entry['text']}\")\n",
        "        print(f\"Model Reply: {entry['response']}\")\n",
        "        print(\"Feedback: Marked as inaccurate\")\n",
        "\n",
        "print(\"\\n--- Product Management Notes ---\")\n",
        "print(\"- Use user feedback to improve prompt strategy or model choice\")\n",
        "print(\"- Show confidence scores in product UX to guide trust\")\n",
        "print(\"- Embed fallback logic or recovery actions for low-confidence cases\")\n",
        "print(\"- Plan iteration cycles based on flagged patterns\")\n"
      ]
    }
  ]
}