{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7QJcNTzpJs_",
        "outputId": "a630cbc9-06bd-4c29-b928-0516e5ba484f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
            "pulling aeda25e63ebd: 100% ‚ñï‚ñè 3.3 GB                         \u001b[K\n",
            "pulling e0a42594d802: 100% ‚ñï‚ñè  358 B                         \u001b[K\n",
            "pulling dd084c7d92a3: 100% ‚ñï‚ñè 8.4 KB                         \u001b[K\n",
            "pulling 3116c5225075: 100% ‚ñï‚ñè   77 B                         \u001b[K\n",
            "pulling b6ae5839783f: 100% ‚ñï‚ñè  489 B                         \u001b[K\n",
            "verifying sha256 digest \u001b[K\n",
            "writing manifest \u001b[K\n",
            "success \u001b[K\u001b[?25h\u001b[?2026l\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.56)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.38)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.25)\n",
            "Requirement already satisfied: langgraph-prebuilt>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.66)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (4.13.2)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.17)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.3.1)\n",
            "Interaction 1:\n",
            "Agent: The capital of France is **Paris**. \n",
            "\n",
            "It's a global center for art, fashion, gastronomy, and culture. üòä \n",
            "\n",
            "Do you want to know anything more about Paris?\n",
            "\n",
            "Interaction 2:\n",
            "Agent: The capital of France is Paris. üòä\n",
            "\n",
            "--- Memory Trace ---\n",
            "Human: What's the capital of France?\n",
            "AI: The capital of France is **Paris**. \n",
            "\n",
            "It's a global center for art, fashion, gastronomy, and culture. üòä \n",
            "\n",
            "Do you want to know anything more about Paris?\n",
            "Human: Remind me what you just told me.\n",
            "AI: The capital of France is Paris. üòä\n"
          ]
        }
      ],
      "source": [
        "# AI Agent with Memory using Ollama and Local Storage\n",
        "\n",
        "\n",
        "# Install Ollama in Colab\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Start Ollama server\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(10)  # wait for Ollama server to start\n",
        "\n",
        "# Pull the Gemma 3  model (small and efficient)\n",
        "!ollama pull gemma3\n",
        "\n",
        "time.sleep(10)\n",
        "\n",
        "# AI Agent with Local Memory and Ollama (LangGraph + LangChain)\n",
        "\n",
        "# AI Agent with Local Memory using Ollama + LangChain + LangGraph\n",
        "\n",
        "# 1. Install Required Packages\n",
        "!pip install langchain langgraph requests\n",
        "\n",
        "# 2. Imports\n",
        "import json\n",
        "import requests\n",
        "from typing import Optional, List, TypedDict\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# 3. Ollama LLM Wrapper with Streaming Fix\n",
        "class OllamaLLM(LLM):\n",
        "    model: str = \"gemma3\"\n",
        "    temperature: float = 0.0\n",
        "    endpoint: str = \"http://localhost:11434\"  # Ensure Ollama is running locally\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        response = requests.post(\n",
        "            f\"{self.endpoint}/api/generate\",\n",
        "            json={\"model\": self.model, \"prompt\": prompt, \"temperature\": self.temperature},\n",
        "            stream=True\n",
        "        )\n",
        "\n",
        "        output = \"\"\n",
        "        for line in response.iter_lines():\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                data = json.loads(line.decode(\"utf-8\"))\n",
        "                if \"response\" in data:\n",
        "                    output += data[\"response\"]\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "\n",
        "        return output.strip()\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"ollama\"\n",
        "\n",
        "llm = OllamaLLM()\n",
        "\n",
        "# 4. Local Memory Implementation\n",
        "class LocalMemory:\n",
        "    def __init__(self):\n",
        "        self.messages = []\n",
        "\n",
        "    def load(self):\n",
        "        return self.messages.copy()\n",
        "\n",
        "    def save(self, input_text, output_text):\n",
        "        self.messages.append(HumanMessage(content=input_text))\n",
        "        self.messages.append(AIMessage(content=output_text))\n",
        "\n",
        "memory = LocalMemory()\n",
        "\n",
        "# 5. Define Agent State Schema\n",
        "class AgentState(TypedDict):\n",
        "    input: str\n",
        "    output: str\n",
        "\n",
        "# 6. Response Function\n",
        "def respond_with_memory(state: AgentState) -> AgentState:\n",
        "    history = memory.load()\n",
        "    input_text = state[\"input\"]\n",
        "    prompt = \"\\n\".join([m.content for m in history] + [input_text])\n",
        "    response = llm(prompt)\n",
        "    memory.save(input_text, response)\n",
        "    return {\"output\": response, \"input\": input_text}\n",
        "\n",
        "# 7. Build LangGraph Workflow\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"respond\", respond_with_memory)\n",
        "workflow.set_entry_point(\"respond\")\n",
        "workflow.set_finish_point(\"respond\")\n",
        "\n",
        "agent = workflow.compile()\n",
        "\n",
        "# 8. Run the Agent\n",
        "print(\"Interaction 1:\")\n",
        "state = {\"input\": \"What's the capital of France?\"}\n",
        "result = agent.invoke(state)\n",
        "print(\"Agent:\", result[\"output\"])\n",
        "\n",
        "print(\"\\nInteraction 2:\")\n",
        "state = {\"input\": \"Remind me what you just told me.\"}\n",
        "result = agent.invoke(state)\n",
        "print(\"Agent:\", result[\"output\"])\n",
        "\n",
        "# 9. View Memory History\n",
        "print(\"\\n--- Memory Trace ---\")\n",
        "for msg in memory.load():\n",
        "    role = \"Human\" if isinstance(msg, HumanMessage) else \"AI\"\n",
        "    print(f\"{role}: {msg.content}\")\n",
        "\n"
      ]
    }
  ]
}